{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Decentralized Stochastic Gradient Langevin Diffusion This package implements the Decentralized Stochastic Gradient Langevin Diffusion Algorithm for both the Vanila and EXTRA methods. DeSGLD is the class that implements the algorithm. DeSGLD Decentralized Stochastic Gradient Langevin Dynamics Parameters: size_w ( int ) \u2013 the size of the network N ( int ) \u2013 the size of the array in each it sigma ( float ) \u2013 the variation of the noise eta ( float ) \u2013 the learning rate T ( int ) \u2013 the iteration numbers dim ( int ) \u2013 the dimension of the input data b ( int ) \u2013 the batch size lam ( float ) \u2013 the regularization parameter x ( float ) \u2013 the input data y ( float ) \u2013 the output data w ( float ) \u2013 the weight matrix from the network structure hv ( float ) \u2013 the tuning parameter for the extra algorithm reg_type ( str ) \u2013 the type of regressor used Returns: \u2013 vanila_desgld() \u2013 extra_desgld() Source code in src/desgld/desgld_alg.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class DeSGLD : \"\"\"Decentralized Stochastic Gradient Langevin Dynamics Args: size_w (int): the size of the network N (int): the size of the array in each it sigma (float): the variation of the noise eta (float): the learning rate T (int): the iteration numbers dim (int): the dimension of the input data b (int): the batch size lam (float): the regularization parameter x (float): the input data y (float): the output data w (float): the weight matrix from the network structure hv (float): the tuning parameter for the extra algorithm reg_type (str): the type of regressor used Returns: vanila_desgld() extra_desgld() \"\"\" def __init__ ( self , size_w , N , sigma , eta , T , dim , b , lam , x , y , w , hv , reg_type ): self . size_w = size_w self . N = N self . sigma = sigma self . eta = eta self . dim = dim self . b = b self . lam = lam self . x = x self . y = y self . w = w self . hv = hv self . T = T self . reg_type = reg_type def gradient_logreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the logistic regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for item in randomList : h = 1 / ( 1 + np . exp ( - np . dot ( beta , x [ item ]))) f -= np . dot (( y [ item ] - h ), x [ item ]) f += ( 2 / lam ) * beta return f def gradient_linreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the linear regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for i in randomList : f = f - np . dot (( y [ i ] - np . dot ( beta , x [ i ])), x [ i ]) f += ( 2 / lam ) * beta return f def vanila_desgld ( self ): \"\"\"Vanila DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all = [] beta_mean_all = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) return np . array ( history_all ), np . array ( beta_mean_all ) def extra_desgld ( self ): \"\"\"EXTRA DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # I_n is the identity matrix of the same size as the weight matrix I_n = np . eye ( self . size_w ) h_values = self . hv history_all = [] beta_mean_all = [] for h in h_values : # w1 is the weight matrix with the hyperparameter h w1 = h * I_n + ( 1 - h ) * self . w # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all_h = [] beta_mean_all_h = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : # Vanila Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : # Vanila Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) history_all . append ( history_all_h ) beta_mean_all . append ( beta_mean_all_h ) return np . array ( history_all ), np . array ( beta_mean_all ) extra_desgld () EXTRA DeSGLD algorithm Returns: history_all \u2013 contains the approximation from all the nodes beta_mean_all \u2013 contains the mean of the approximation from \u2013 all the nodes Source code in src/desgld/desgld_alg.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def extra_desgld ( self ): \"\"\"EXTRA DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # I_n is the identity matrix of the same size as the weight matrix I_n = np . eye ( self . size_w ) h_values = self . hv history_all = [] beta_mean_all = [] for h in h_values : # w1 is the weight matrix with the hyperparameter h w1 = h * I_n + ( 1 - h ) * self . w # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all_h = [] beta_mean_all_h = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : # Vanila Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : # Vanila Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) history_all . append ( history_all_h ) beta_mean_all . append ( beta_mean_all_h ) return np . array ( history_all ), np . array ( beta_mean_all ) gradient_linreg ( beta , x , y , dim , lam , b ) Gradient function for the linear regression Parameters: beta \u2013 approximation at each node x \u2013 the input data y \u2013 the output data dim \u2013 the dimension of the input data lam \u2013 the regularization parameter b \u2013 the batch size Source code in src/desgld/desgld_alg.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def gradient_linreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the linear regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for i in randomList : f = f - np . dot (( y [ i ] - np . dot ( beta , x [ i ])), x [ i ]) f += ( 2 / lam ) * beta return f gradient_logreg ( beta , x , y , dim , lam , b ) Gradient function for the logistic regression Parameters: beta \u2013 approximation at each node x \u2013 the input data y \u2013 the output data dim \u2013 the dimension of the input data lam \u2013 the regularization parameter b \u2013 the batch size Source code in src/desgld/desgld_alg.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def gradient_logreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the logistic regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for item in randomList : h = 1 / ( 1 + np . exp ( - np . dot ( beta , x [ item ]))) f -= np . dot (( y [ item ] - h ), x [ item ]) f += ( 2 / lam ) * beta return f vanila_desgld () Vanila DeSGLD algorithm Returns: history_all \u2013 contains the approximation from all the nodes beta_mean_all \u2013 contains the mean of the approximation from \u2013 all the nodes Source code in src/desgld/desgld_alg.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def vanila_desgld ( self ): \"\"\"Vanila DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all = [] beta_mean_all = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) return np . array ( history_all ), np . array ( beta_mean_all )","title":"Decentralized Stochastic Gradient Langevin Diffusion"},{"location":"#decentralized-stochastic-gradient-langevin-diffusion","text":"This package implements the Decentralized Stochastic Gradient Langevin Diffusion Algorithm for both the Vanila and EXTRA methods. DeSGLD is the class that implements the algorithm.","title":"Decentralized Stochastic Gradient Langevin Diffusion"},{"location":"#src.desgld.desgld_alg.DeSGLD","text":"Decentralized Stochastic Gradient Langevin Dynamics Parameters: size_w ( int ) \u2013 the size of the network N ( int ) \u2013 the size of the array in each it sigma ( float ) \u2013 the variation of the noise eta ( float ) \u2013 the learning rate T ( int ) \u2013 the iteration numbers dim ( int ) \u2013 the dimension of the input data b ( int ) \u2013 the batch size lam ( float ) \u2013 the regularization parameter x ( float ) \u2013 the input data y ( float ) \u2013 the output data w ( float ) \u2013 the weight matrix from the network structure hv ( float ) \u2013 the tuning parameter for the extra algorithm reg_type ( str ) \u2013 the type of regressor used Returns: \u2013 vanila_desgld() \u2013 extra_desgld() Source code in src/desgld/desgld_alg.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class DeSGLD : \"\"\"Decentralized Stochastic Gradient Langevin Dynamics Args: size_w (int): the size of the network N (int): the size of the array in each it sigma (float): the variation of the noise eta (float): the learning rate T (int): the iteration numbers dim (int): the dimension of the input data b (int): the batch size lam (float): the regularization parameter x (float): the input data y (float): the output data w (float): the weight matrix from the network structure hv (float): the tuning parameter for the extra algorithm reg_type (str): the type of regressor used Returns: vanila_desgld() extra_desgld() \"\"\" def __init__ ( self , size_w , N , sigma , eta , T , dim , b , lam , x , y , w , hv , reg_type ): self . size_w = size_w self . N = N self . sigma = sigma self . eta = eta self . dim = dim self . b = b self . lam = lam self . x = x self . y = y self . w = w self . hv = hv self . T = T self . reg_type = reg_type def gradient_logreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the logistic regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for item in randomList : h = 1 / ( 1 + np . exp ( - np . dot ( beta , x [ item ]))) f -= np . dot (( y [ item ] - h ), x [ item ]) f += ( 2 / lam ) * beta return f def gradient_linreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the linear regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for i in randomList : f = f - np . dot (( y [ i ] - np . dot ( beta , x [ i ])), x [ i ]) f += ( 2 / lam ) * beta return f def vanila_desgld ( self ): \"\"\"Vanila DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all = [] beta_mean_all = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) return np . array ( history_all ), np . array ( beta_mean_all ) def extra_desgld ( self ): \"\"\"EXTRA DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # I_n is the identity matrix of the same size as the weight matrix I_n = np . eye ( self . size_w ) h_values = self . hv history_all = [] beta_mean_all = [] for h in h_values : # w1 is the weight matrix with the hyperparameter h w1 = h * I_n + ( 1 - h ) * self . w # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all_h = [] beta_mean_all_h = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : # Vanila Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : # Vanila Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) history_all . append ( history_all_h ) beta_mean_all . append ( beta_mean_all_h ) return np . array ( history_all ), np . array ( beta_mean_all )","title":"DeSGLD"},{"location":"#src.desgld.desgld_alg.DeSGLD.extra_desgld","text":"EXTRA DeSGLD algorithm Returns: history_all \u2013 contains the approximation from all the nodes beta_mean_all \u2013 contains the mean of the approximation from \u2013 all the nodes Source code in src/desgld/desgld_alg.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def extra_desgld ( self ): \"\"\"EXTRA DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # I_n is the identity matrix of the same size as the weight matrix I_n = np . eye ( self . size_w ) h_values = self . hv history_all = [] beta_mean_all = [] for h in h_values : # w1 is the weight matrix with the hyperparameter h w1 = h * I_n + ( 1 - h ) * self . w # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all_h = [] beta_mean_all_h = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : # Vanila Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : # Vanila Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) history_all . append ( history_all_h ) beta_mean_all . append ( beta_mean_all_h ) return np . array ( history_all ), np . array ( beta_mean_all )","title":"extra_desgld"},{"location":"#src.desgld.desgld_alg.DeSGLD.gradient_linreg","text":"Gradient function for the linear regression Parameters: beta \u2013 approximation at each node x \u2013 the input data y \u2013 the output data dim \u2013 the dimension of the input data lam \u2013 the regularization parameter b \u2013 the batch size Source code in src/desgld/desgld_alg.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def gradient_linreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the linear regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for i in randomList : f = f - np . dot (( y [ i ] - np . dot ( beta , x [ i ])), x [ i ]) f += ( 2 / lam ) * beta return f","title":"gradient_linreg"},{"location":"#src.desgld.desgld_alg.DeSGLD.gradient_logreg","text":"Gradient function for the logistic regression Parameters: beta \u2013 approximation at each node x \u2013 the input data y \u2013 the output data dim \u2013 the dimension of the input data lam \u2013 the regularization parameter b \u2013 the batch size Source code in src/desgld/desgld_alg.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def gradient_logreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the logistic regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for item in randomList : h = 1 / ( 1 + np . exp ( - np . dot ( beta , x [ item ]))) f -= np . dot (( y [ item ] - h ), x [ item ]) f += ( 2 / lam ) * beta return f","title":"gradient_logreg"},{"location":"#src.desgld.desgld_alg.DeSGLD.vanila_desgld","text":"Vanila DeSGLD algorithm Returns: history_all \u2013 contains the approximation from all the nodes beta_mean_all \u2013 contains the mean of the approximation from \u2013 all the nodes Source code in src/desgld/desgld_alg.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def vanila_desgld ( self ): \"\"\"Vanila DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all = [] beta_mean_all = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) return np . array ( history_all ), np . array ( beta_mean_all )","title":"vanila_desgld"},{"location":"about/","text":"Package Details name = desgld , version = 0.1.0 url = https : //github.com/mrislambd/desgld_package.git contact : mi21b @ fsu . edu Requirements This package requires the following libraries numpy scipy scikit-learn matplotlib tqdm Development Requirements This package requires the following dev requirements [\"black\", \"isort\", \"flake8\", \"pre-commit\"]","title":"Package"},{"location":"about/#package-details","text":"name = desgld , version = 0.1.0 url = https : //github.com/mrislambd/desgld_package.git contact : mi21b @ fsu . edu","title":"Package Details"},{"location":"about/#requirements","text":"This package requires the following libraries numpy scipy scikit-learn matplotlib tqdm","title":"Requirements"},{"location":"about/#development-requirements","text":"This package requires the following dev requirements [\"black\", \"isort\", \"flake8\", \"pre-commit\"]","title":"Development Requirements"},{"location":"desgld/","text":"Decentralized Stochastic Gradient Langevin Diffusion This package implements the Decentralized Stochastic Gradient Langevin Diffusion Algorithm for both the Vanila and EXTRA methods. DeSGLD is the class that implements the algorithm. DeSGLD Decentralized Stochastic Gradient Langevin Dynamics Parameters: size_w ( int ) \u2013 the size of the network N ( int ) \u2013 the size of the array in each it sigma ( float ) \u2013 the variation of the noise eta ( float ) \u2013 the learning rate T ( int ) \u2013 the iteration numbers dim ( int ) \u2013 the dimension of the input data b ( int ) \u2013 the batch size lam ( float ) \u2013 the regularization parameter x ( float ) \u2013 the input data y ( float ) \u2013 the output data w ( float ) \u2013 the weight matrix from the network structure hv ( float ) \u2013 the tuning parameter for the extra algorithm reg_type ( str ) \u2013 the type of regressor used Returns: \u2013 vanila_desgld() \u2013 extra_desgld() Source code in src/desgld/desgld_alg.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class DeSGLD : \"\"\"Decentralized Stochastic Gradient Langevin Dynamics Args: size_w (int): the size of the network N (int): the size of the array in each it sigma (float): the variation of the noise eta (float): the learning rate T (int): the iteration numbers dim (int): the dimension of the input data b (int): the batch size lam (float): the regularization parameter x (float): the input data y (float): the output data w (float): the weight matrix from the network structure hv (float): the tuning parameter for the extra algorithm reg_type (str): the type of regressor used Returns: vanila_desgld() extra_desgld() \"\"\" def __init__ ( self , size_w , N , sigma , eta , T , dim , b , lam , x , y , w , hv , reg_type ): self . size_w = size_w self . N = N self . sigma = sigma self . eta = eta self . dim = dim self . b = b self . lam = lam self . x = x self . y = y self . w = w self . hv = hv self . T = T self . reg_type = reg_type def gradient_logreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the logistic regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for item in randomList : h = 1 / ( 1 + np . exp ( - np . dot ( beta , x [ item ]))) f -= np . dot (( y [ item ] - h ), x [ item ]) f += ( 2 / lam ) * beta return f def gradient_linreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the linear regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for i in randomList : f = f - np . dot (( y [ i ] - np . dot ( beta , x [ i ])), x [ i ]) f += ( 2 / lam ) * beta return f def vanila_desgld ( self ): \"\"\"Vanila DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all = [] beta_mean_all = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) return np . array ( history_all ), np . array ( beta_mean_all ) def extra_desgld ( self ): \"\"\"EXTRA DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # I_n is the identity matrix of the same size as the weight matrix I_n = np . eye ( self . size_w ) h_values = self . hv history_all = [] beta_mean_all = [] for h in h_values : # w1 is the weight matrix with the hyperparameter h w1 = h * I_n + ( 1 - h ) * self . w # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all_h = [] beta_mean_all_h = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : # Vanila Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : # Vanila Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) history_all . append ( history_all_h ) beta_mean_all . append ( beta_mean_all_h ) return np . array ( history_all ), np . array ( beta_mean_all ) extra_desgld () EXTRA DeSGLD algorithm Returns: history_all \u2013 contains the approximation from all the nodes beta_mean_all \u2013 contains the mean of the approximation from \u2013 all the nodes Source code in src/desgld/desgld_alg.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def extra_desgld ( self ): \"\"\"EXTRA DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # I_n is the identity matrix of the same size as the weight matrix I_n = np . eye ( self . size_w ) h_values = self . hv history_all = [] beta_mean_all = [] for h in h_values : # w1 is the weight matrix with the hyperparameter h w1 = h * I_n + ( 1 - h ) * self . w # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all_h = [] beta_mean_all_h = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : # Vanila Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : # Vanila Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) history_all . append ( history_all_h ) beta_mean_all . append ( beta_mean_all_h ) return np . array ( history_all ), np . array ( beta_mean_all ) gradient_linreg ( beta , x , y , dim , lam , b ) Gradient function for the linear regression Parameters: beta \u2013 approximation at each node x \u2013 the input data y \u2013 the output data dim \u2013 the dimension of the input data lam \u2013 the regularization parameter b \u2013 the batch size Source code in src/desgld/desgld_alg.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def gradient_linreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the linear regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for i in randomList : f = f - np . dot (( y [ i ] - np . dot ( beta , x [ i ])), x [ i ]) f += ( 2 / lam ) * beta return f gradient_logreg ( beta , x , y , dim , lam , b ) Gradient function for the logistic regression Parameters: beta \u2013 approximation at each node x \u2013 the input data y \u2013 the output data dim \u2013 the dimension of the input data lam \u2013 the regularization parameter b \u2013 the batch size Source code in src/desgld/desgld_alg.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def gradient_logreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the logistic regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for item in randomList : h = 1 / ( 1 + np . exp ( - np . dot ( beta , x [ item ]))) f -= np . dot (( y [ item ] - h ), x [ item ]) f += ( 2 / lam ) * beta return f vanila_desgld () Vanila DeSGLD algorithm Returns: history_all \u2013 contains the approximation from all the nodes beta_mean_all \u2013 contains the mean of the approximation from \u2013 all the nodes Source code in src/desgld/desgld_alg.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def vanila_desgld ( self ): \"\"\"Vanila DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all = [] beta_mean_all = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) return np . array ( history_all ), np . array ( beta_mean_all )","title":"DeSGLD"},{"location":"desgld/#decentralized-stochastic-gradient-langevin-diffusion","text":"This package implements the Decentralized Stochastic Gradient Langevin Diffusion Algorithm for both the Vanila and EXTRA methods. DeSGLD is the class that implements the algorithm.","title":"Decentralized Stochastic Gradient Langevin Diffusion"},{"location":"desgld/#src.desgld.desgld_alg.DeSGLD","text":"Decentralized Stochastic Gradient Langevin Dynamics Parameters: size_w ( int ) \u2013 the size of the network N ( int ) \u2013 the size of the array in each it sigma ( float ) \u2013 the variation of the noise eta ( float ) \u2013 the learning rate T ( int ) \u2013 the iteration numbers dim ( int ) \u2013 the dimension of the input data b ( int ) \u2013 the batch size lam ( float ) \u2013 the regularization parameter x ( float ) \u2013 the input data y ( float ) \u2013 the output data w ( float ) \u2013 the weight matrix from the network structure hv ( float ) \u2013 the tuning parameter for the extra algorithm reg_type ( str ) \u2013 the type of regressor used Returns: \u2013 vanila_desgld() \u2013 extra_desgld() Source code in src/desgld/desgld_alg.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class DeSGLD : \"\"\"Decentralized Stochastic Gradient Langevin Dynamics Args: size_w (int): the size of the network N (int): the size of the array in each it sigma (float): the variation of the noise eta (float): the learning rate T (int): the iteration numbers dim (int): the dimension of the input data b (int): the batch size lam (float): the regularization parameter x (float): the input data y (float): the output data w (float): the weight matrix from the network structure hv (float): the tuning parameter for the extra algorithm reg_type (str): the type of regressor used Returns: vanila_desgld() extra_desgld() \"\"\" def __init__ ( self , size_w , N , sigma , eta , T , dim , b , lam , x , y , w , hv , reg_type ): self . size_w = size_w self . N = N self . sigma = sigma self . eta = eta self . dim = dim self . b = b self . lam = lam self . x = x self . y = y self . w = w self . hv = hv self . T = T self . reg_type = reg_type def gradient_logreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the logistic regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for item in randomList : h = 1 / ( 1 + np . exp ( - np . dot ( beta , x [ item ]))) f -= np . dot (( y [ item ] - h ), x [ item ]) f += ( 2 / lam ) * beta return f def gradient_linreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the linear regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for i in randomList : f = f - np . dot (( y [ i ] - np . dot ( beta , x [ i ])), x [ i ]) f += ( 2 / lam ) * beta return f def vanila_desgld ( self ): \"\"\"Vanila DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all = [] beta_mean_all = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) return np . array ( history_all ), np . array ( beta_mean_all ) def extra_desgld ( self ): \"\"\"EXTRA DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # I_n is the identity matrix of the same size as the weight matrix I_n = np . eye ( self . size_w ) h_values = self . hv history_all = [] beta_mean_all = [] for h in h_values : # w1 is the weight matrix with the hyperparameter h w1 = h * I_n + ( 1 - h ) * self . w # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all_h = [] beta_mean_all_h = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : # Vanila Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : # Vanila Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) history_all . append ( history_all_h ) beta_mean_all . append ( beta_mean_all_h ) return np . array ( history_all ), np . array ( beta_mean_all )","title":"DeSGLD"},{"location":"desgld/#src.desgld.desgld_alg.DeSGLD.extra_desgld","text":"EXTRA DeSGLD algorithm Returns: history_all \u2013 contains the approximation from all the nodes beta_mean_all \u2013 contains the mean of the approximation from \u2013 all the nodes Source code in src/desgld/desgld_alg.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def extra_desgld ( self ): \"\"\"EXTRA DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # I_n is the identity matrix of the same size as the weight matrix I_n = np . eye ( self . size_w ) h_values = self . hv history_all = [] beta_mean_all = [] for h in h_values : # w1 is the weight matrix with the hyperparameter h w1 = h * I_n + ( 1 - h ) * self . w # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all_h = [] beta_mean_all_h = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : # Vanila Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : # Vanila Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + w1 [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) # Extra Part g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all_h . append ( history ) beta_mean_all_h . append ( beta_mean ) history_all . append ( history_all_h ) beta_mean_all . append ( beta_mean_all_h ) return np . array ( history_all ), np . array ( beta_mean_all )","title":"extra_desgld"},{"location":"desgld/#src.desgld.desgld_alg.DeSGLD.gradient_linreg","text":"Gradient function for the linear regression Parameters: beta \u2013 approximation at each node x \u2013 the input data y \u2013 the output data dim \u2013 the dimension of the input data lam \u2013 the regularization parameter b \u2013 the batch size Source code in src/desgld/desgld_alg.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def gradient_linreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the linear regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for i in randomList : f = f - np . dot (( y [ i ] - np . dot ( beta , x [ i ])), x [ i ]) f += ( 2 / lam ) * beta return f","title":"gradient_linreg"},{"location":"desgld/#src.desgld.desgld_alg.DeSGLD.gradient_logreg","text":"Gradient function for the logistic regression Parameters: beta \u2013 approximation at each node x \u2013 the input data y \u2013 the output data dim \u2013 the dimension of the input data lam \u2013 the regularization parameter b \u2013 the batch size Source code in src/desgld/desgld_alg.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def gradient_logreg ( self , beta , x , y , dim , lam , b ): \"\"\"Gradient function for the logistic regression Args: beta: approximation at each node x: the input data y: the output data dim: the dimension of the input data lam: the regularization parameter b: the batch size Returns: gradient for the the logistic regression \"\"\" f = np . zeros ( dim ) randomList = np . random . randint ( 0 , len ( y ) - 1 , size = int ( b )) for item in randomList : h = 1 / ( 1 + np . exp ( - np . dot ( beta , x [ item ]))) f -= np . dot (( y [ item ] - h ), x [ item ]) f += ( 2 / lam ) * beta return f","title":"gradient_logreg"},{"location":"desgld/#src.desgld.desgld_alg.DeSGLD.vanila_desgld","text":"Vanila DeSGLD algorithm Returns: history_all \u2013 contains the approximation from all the nodes beta_mean_all \u2013 contains the mean of the approximation from \u2013 all the nodes Source code in src/desgld/desgld_alg.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def vanila_desgld ( self ): \"\"\"Vanila DeSGLD algorithm Returns: history_all: contains the approximation from all the nodes beta_mean_all: contains the mean of the approximation from all the nodes \"\"\" # Initialization if self . reg_type == \"logistic\" : beta = np . random . normal ( 0 , self . sigma , size = ( self . N , self . size_w , self . dim ) ) else : beta = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ), size = ( self . N , self . size_w ), ) history_all = [] beta_mean_all = [] for t in range ( 1 ): history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) # Update step = self . eta for t in range ( self . T ): for n in range ( self . N ): for i in range ( self . size_w ): if self . reg_type == \"logistic\" : g = self . gradient_logreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . normal ( 0 , self . sigma , self . dim ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) else : g = self . gradient_linreg ( beta [ n , i ], self . x [ i ], self . y [ i ], self . dim , self . lam , self . b , ) temp = np . zeros ( self . dim ) for j in range ( len ( beta [ n ])): temp = temp + self . w [ i , j ] * beta [ n , j ] noise = np . random . multivariate_normal ( mean = np . zeros ( self . dim ), cov = np . eye ( self . dim ) ) beta [ n , i ] = ( temp - step * g + math . sqrt ( 2 * step ) * noise ) history = np . empty (( self . size_w , self . dim , self . N )) beta_mean = np . empty (( self . dim , self . N )) for i in range ( self . N ): history [:, :, i ] = beta [ i , :, :] for j in range ( self . dim ): beta_mean [ j , :] = np . mean ( history [:, j , :], axis = 0 ) history_all . append ( history ) beta_mean_all . append ( beta_mean ) return np . array ( history_all ), np . array ( beta_mean_all )","title":"vanila_desgld"},{"location":"evaluation/","text":"Evaluation of the Approximations This Module provides the detailes of the evaluation of the approximation from both of the regressions. For the Bayesian logistic regression we use the ClassificationAccuracy class and for the Bayesian linear regression we use the the Wasserstein2Distance class. ClassificationAccuracy Calculate the classification accuracy in Bayesian Logistic Regression Parameters: x_all ( list ) \u2013 the input data y_all ( list ) \u2013 the output data history_all ( list ) \u2013 contains the approximation from all the nodes T ( int ) \u2013 the number of iterations Source code in src/desgld/evaluation.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class ClassificationAccuracy : \"\"\"Calculate the classification accuracy in Bayesian Logistic Regression Args: x_all (list): the input data y_all (list): the output data history_all (list): contains the approximation from all the nodes T (int): the number of iterations \"\"\" def __init__ ( self , x_all , y_all , history_all , T ): self . x_all = x_all self . y_all = y_all self . history_all = history_all self . T = T def compute_accuracy ( self ): \"\"\"Function that computes the classification accuracy Args: params: the same parameters from the class Returns: result_acc (float): the classification accuracy result_std (float): the standard deviation of the classification accuracy \"\"\" mis_class = np . empty (( self . T + 1 , len ( self . history_all [ 0 , 0 , 0 ]))) for t in range ( self . T + 1 ): for n in range ( len ( self . history_all [ t , 0 , 0 ])): temp0 = 0 for i in range ( len ( self . x_all )): z = 1 / ( 1 + np . exp ( - np . dot ( np . transpose ( self . history_all [ t , 1 ])[ n ], self . x_all [ i ], ) ) ) if z >= 0.5 : z = 1 else : z = 0 if self . y_all [ i ] != z : temp0 += 1 mis_class [ t , n ] = 1 - temp0 / len ( self . x_all ) result_acc = np . mean ( mis_class , axis = 1 ) result_std = np . std ( mis_class , axis = 1 ) return result_acc , result_std compute_accuracy () Function that computes the classification accuracy Parameters: params \u2013 the same parameters from the class result_acc (float): the classification accuracy result_std (float): the standard deviation of the classification accuracy Source code in src/desgld/evaluation.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def compute_accuracy ( self ): \"\"\"Function that computes the classification accuracy Args: params: the same parameters from the class Returns: result_acc (float): the classification accuracy result_std (float): the standard deviation of the classification accuracy \"\"\" mis_class = np . empty (( self . T + 1 , len ( self . history_all [ 0 , 0 , 0 ]))) for t in range ( self . T + 1 ): for n in range ( len ( self . history_all [ t , 0 , 0 ])): temp0 = 0 for i in range ( len ( self . x_all )): z = 1 / ( 1 + np . exp ( - np . dot ( np . transpose ( self . history_all [ t , 1 ])[ n ], self . x_all [ i ], ) ) ) if z >= 0.5 : z = 1 else : z = 0 if self . y_all [ i ] != z : temp0 += 1 mis_class [ t , n ] = 1 - temp0 / len ( self . x_all ) result_acc = np . mean ( mis_class , axis = 1 ) result_std = np . std ( mis_class , axis = 1 ) return result_acc , result_std Wasserstein2Distance Class for: Wasserstein 2 distance in Bayesian Linear Regression Parameters: size_w ( int ) \u2013 the size of the network T ( int ) \u2013 the number of iterations avg_post ( list ) \u2013 the mean of the posterior distribution cov_post ( list ) \u2013 the covariance of the posterior distribution history_all ( list ) \u2013 contains the approximation from all the nodes beta_mean_all ( list ) \u2013 contains the mean of the approximation Source code in src/desgld/evaluation.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class Wasserstein2Distance : \"\"\"Class for: Wasserstein 2 distance in Bayesian Linear Regression Args: size_w (int): the size of the network T (int): the number of iterations avg_post (list): the mean of the posterior distribution cov_post (list): the covariance of the posterior distribution history_all (list): contains the approximation from all the nodes beta_mean_all (list): contains the mean of the approximation from all the nodes \"\"\" def __init__ ( self , size_w , T , avg_post , cov_post , history_all , beta_mean_all ): self . size_w = size_w self . T = T self . avg_post = avg_post self . cov_post = cov_post self . history_all = history_all self . beta_mean_all = beta_mean_all def W2_dist ( self ): \"\"\"Class for: Wasserstein 2 distance in Bayesian Linear Regression Args: size_w (int): the size of the network T (int): the number of iterations avg_post (list): the mean of the posterior distribution cov_post (list): the covariance of the posterior distribution history_all (list): contains the approximation from all the nodes beta_mean_all (list): contains the mean of the approximation from all the nodes Returns: w2dis (list): contains the W2 distance of each agent and the mean of the approximation from all the ageents \"\"\" w2dis = [] for i in range ( self . size_w ): temp = [] w2dis . append ( temp ) temp = [] w2dis . append ( temp ) \"\"\" W2 distance of each agent \"\"\" for i in range ( self . size_w ): for t in range ( self . T + 1 ): d = 0 avg_temp = [] avg_temp . append ( np . mean ( self . history_all [ t ][ i ][ 0 ])) avg_temp . append ( np . mean ( self . history_all [ t ][ i ][ 1 ])) avg_temp = np . array ( avg_temp ) cov_temp = np . cov ( self . history_all [ t ][ i ]) d = np . linalg . norm ( self . avg_post - avg_temp ) * np . linalg . norm ( self . avg_post - avg_temp ) d = d + np . trace ( self . cov_post + cov_temp - 2 * sqrtm ( np . dot ( np . dot ( sqrtm ( cov_temp ), self . cov_post ), sqrtm ( cov_temp ), ) ) ) w2dis [ i ] . append ( np . array ( math . sqrt ( abs ( d )))) \"\"\" W2 distance of the mean of agents \"\"\" for t in range ( self . T + 1 ): d = 0 avg_temp = [] avg_temp . append ( np . mean ( self . beta_mean_all [ t ][ 0 ])) avg_temp . append ( np . mean ( self . beta_mean_all [ t ][ 1 ])) avg_temp = np . array ( avg_temp ) cov_temp = np . cov ( self . beta_mean_all [ t ]) d = np . linalg . norm ( self . avg_post - avg_temp ) * np . linalg . norm ( self . avg_post - avg_temp ) d = d + np . trace ( self . cov_post + cov_temp - 2 * sqrtm ( np . dot ( np . dot ( sqrtm ( cov_temp ), self . cov_post ), sqrtm ( cov_temp ) ) ) ) w2dis [ self . size_w ] . append ( np . array ( math . sqrt ( abs ( d )))) for i in range ( len ( w2dis )): w2dis [ i ] = np . array ( w2dis [ i ]) return w2dis W2_dist () Class for: Wasserstein 2 distance in Bayesian Linear Regression Parameters: size_w ( int ) \u2013 the size of the network T ( int ) \u2013 the number of iterations avg_post ( list ) \u2013 the mean of the posterior distribution cov_post ( list ) \u2013 the covariance of the posterior distribution history_all ( list ) \u2013 contains the approximation from all the nodes beta_mean_all ( list ) \u2013 contains the mean of the approximation w2dis (list): contains the W2 distance of each agent and the mean of the approximation from all the ageents Source code in src/desgld/evaluation.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def W2_dist ( self ): \"\"\"Class for: Wasserstein 2 distance in Bayesian Linear Regression Args: size_w (int): the size of the network T (int): the number of iterations avg_post (list): the mean of the posterior distribution cov_post (list): the covariance of the posterior distribution history_all (list): contains the approximation from all the nodes beta_mean_all (list): contains the mean of the approximation from all the nodes Returns: w2dis (list): contains the W2 distance of each agent and the mean of the approximation from all the ageents \"\"\" w2dis = [] for i in range ( self . size_w ): temp = [] w2dis . append ( temp ) temp = [] w2dis . append ( temp ) \"\"\" W2 distance of each agent \"\"\" for i in range ( self . size_w ): for t in range ( self . T + 1 ): d = 0 avg_temp = [] avg_temp . append ( np . mean ( self . history_all [ t ][ i ][ 0 ])) avg_temp . append ( np . mean ( self . history_all [ t ][ i ][ 1 ])) avg_temp = np . array ( avg_temp ) cov_temp = np . cov ( self . history_all [ t ][ i ]) d = np . linalg . norm ( self . avg_post - avg_temp ) * np . linalg . norm ( self . avg_post - avg_temp ) d = d + np . trace ( self . cov_post + cov_temp - 2 * sqrtm ( np . dot ( np . dot ( sqrtm ( cov_temp ), self . cov_post ), sqrtm ( cov_temp ), ) ) ) w2dis [ i ] . append ( np . array ( math . sqrt ( abs ( d )))) \"\"\" W2 distance of the mean of agents \"\"\" for t in range ( self . T + 1 ): d = 0 avg_temp = [] avg_temp . append ( np . mean ( self . beta_mean_all [ t ][ 0 ])) avg_temp . append ( np . mean ( self . beta_mean_all [ t ][ 1 ])) avg_temp = np . array ( avg_temp ) cov_temp = np . cov ( self . beta_mean_all [ t ]) d = np . linalg . norm ( self . avg_post - avg_temp ) * np . linalg . norm ( self . avg_post - avg_temp ) d = d + np . trace ( self . cov_post + cov_temp - 2 * sqrtm ( np . dot ( np . dot ( sqrtm ( cov_temp ), self . cov_post ), sqrtm ( cov_temp ) ) ) ) w2dis [ self . size_w ] . append ( np . array ( math . sqrt ( abs ( d )))) for i in range ( len ( w2dis )): w2dis [ i ] = np . array ( w2dis [ i ]) return w2dis","title":"Evaluation"},{"location":"evaluation/#evaluation-of-the-approximations","text":"This Module provides the detailes of the evaluation of the approximation from both of the regressions. For the Bayesian logistic regression we use the ClassificationAccuracy class and for the Bayesian linear regression we use the the Wasserstein2Distance class.","title":"Evaluation of the Approximations"},{"location":"evaluation/#src.desgld.evaluation.ClassificationAccuracy","text":"Calculate the classification accuracy in Bayesian Logistic Regression Parameters: x_all ( list ) \u2013 the input data y_all ( list ) \u2013 the output data history_all ( list ) \u2013 contains the approximation from all the nodes T ( int ) \u2013 the number of iterations Source code in src/desgld/evaluation.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class ClassificationAccuracy : \"\"\"Calculate the classification accuracy in Bayesian Logistic Regression Args: x_all (list): the input data y_all (list): the output data history_all (list): contains the approximation from all the nodes T (int): the number of iterations \"\"\" def __init__ ( self , x_all , y_all , history_all , T ): self . x_all = x_all self . y_all = y_all self . history_all = history_all self . T = T def compute_accuracy ( self ): \"\"\"Function that computes the classification accuracy Args: params: the same parameters from the class Returns: result_acc (float): the classification accuracy result_std (float): the standard deviation of the classification accuracy \"\"\" mis_class = np . empty (( self . T + 1 , len ( self . history_all [ 0 , 0 , 0 ]))) for t in range ( self . T + 1 ): for n in range ( len ( self . history_all [ t , 0 , 0 ])): temp0 = 0 for i in range ( len ( self . x_all )): z = 1 / ( 1 + np . exp ( - np . dot ( np . transpose ( self . history_all [ t , 1 ])[ n ], self . x_all [ i ], ) ) ) if z >= 0.5 : z = 1 else : z = 0 if self . y_all [ i ] != z : temp0 += 1 mis_class [ t , n ] = 1 - temp0 / len ( self . x_all ) result_acc = np . mean ( mis_class , axis = 1 ) result_std = np . std ( mis_class , axis = 1 ) return result_acc , result_std","title":"ClassificationAccuracy"},{"location":"evaluation/#src.desgld.evaluation.ClassificationAccuracy.compute_accuracy","text":"Function that computes the classification accuracy Parameters: params \u2013 the same parameters from the class result_acc (float): the classification accuracy result_std (float): the standard deviation of the classification accuracy Source code in src/desgld/evaluation.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def compute_accuracy ( self ): \"\"\"Function that computes the classification accuracy Args: params: the same parameters from the class Returns: result_acc (float): the classification accuracy result_std (float): the standard deviation of the classification accuracy \"\"\" mis_class = np . empty (( self . T + 1 , len ( self . history_all [ 0 , 0 , 0 ]))) for t in range ( self . T + 1 ): for n in range ( len ( self . history_all [ t , 0 , 0 ])): temp0 = 0 for i in range ( len ( self . x_all )): z = 1 / ( 1 + np . exp ( - np . dot ( np . transpose ( self . history_all [ t , 1 ])[ n ], self . x_all [ i ], ) ) ) if z >= 0.5 : z = 1 else : z = 0 if self . y_all [ i ] != z : temp0 += 1 mis_class [ t , n ] = 1 - temp0 / len ( self . x_all ) result_acc = np . mean ( mis_class , axis = 1 ) result_std = np . std ( mis_class , axis = 1 ) return result_acc , result_std","title":"compute_accuracy"},{"location":"evaluation/#src.desgld.evaluation.Wasserstein2Distance","text":"Class for: Wasserstein 2 distance in Bayesian Linear Regression Parameters: size_w ( int ) \u2013 the size of the network T ( int ) \u2013 the number of iterations avg_post ( list ) \u2013 the mean of the posterior distribution cov_post ( list ) \u2013 the covariance of the posterior distribution history_all ( list ) \u2013 contains the approximation from all the nodes beta_mean_all ( list ) \u2013 contains the mean of the approximation Source code in src/desgld/evaluation.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class Wasserstein2Distance : \"\"\"Class for: Wasserstein 2 distance in Bayesian Linear Regression Args: size_w (int): the size of the network T (int): the number of iterations avg_post (list): the mean of the posterior distribution cov_post (list): the covariance of the posterior distribution history_all (list): contains the approximation from all the nodes beta_mean_all (list): contains the mean of the approximation from all the nodes \"\"\" def __init__ ( self , size_w , T , avg_post , cov_post , history_all , beta_mean_all ): self . size_w = size_w self . T = T self . avg_post = avg_post self . cov_post = cov_post self . history_all = history_all self . beta_mean_all = beta_mean_all def W2_dist ( self ): \"\"\"Class for: Wasserstein 2 distance in Bayesian Linear Regression Args: size_w (int): the size of the network T (int): the number of iterations avg_post (list): the mean of the posterior distribution cov_post (list): the covariance of the posterior distribution history_all (list): contains the approximation from all the nodes beta_mean_all (list): contains the mean of the approximation from all the nodes Returns: w2dis (list): contains the W2 distance of each agent and the mean of the approximation from all the ageents \"\"\" w2dis = [] for i in range ( self . size_w ): temp = [] w2dis . append ( temp ) temp = [] w2dis . append ( temp ) \"\"\" W2 distance of each agent \"\"\" for i in range ( self . size_w ): for t in range ( self . T + 1 ): d = 0 avg_temp = [] avg_temp . append ( np . mean ( self . history_all [ t ][ i ][ 0 ])) avg_temp . append ( np . mean ( self . history_all [ t ][ i ][ 1 ])) avg_temp = np . array ( avg_temp ) cov_temp = np . cov ( self . history_all [ t ][ i ]) d = np . linalg . norm ( self . avg_post - avg_temp ) * np . linalg . norm ( self . avg_post - avg_temp ) d = d + np . trace ( self . cov_post + cov_temp - 2 * sqrtm ( np . dot ( np . dot ( sqrtm ( cov_temp ), self . cov_post ), sqrtm ( cov_temp ), ) ) ) w2dis [ i ] . append ( np . array ( math . sqrt ( abs ( d )))) \"\"\" W2 distance of the mean of agents \"\"\" for t in range ( self . T + 1 ): d = 0 avg_temp = [] avg_temp . append ( np . mean ( self . beta_mean_all [ t ][ 0 ])) avg_temp . append ( np . mean ( self . beta_mean_all [ t ][ 1 ])) avg_temp = np . array ( avg_temp ) cov_temp = np . cov ( self . beta_mean_all [ t ]) d = np . linalg . norm ( self . avg_post - avg_temp ) * np . linalg . norm ( self . avg_post - avg_temp ) d = d + np . trace ( self . cov_post + cov_temp - 2 * sqrtm ( np . dot ( np . dot ( sqrtm ( cov_temp ), self . cov_post ), sqrtm ( cov_temp ) ) ) ) w2dis [ self . size_w ] . append ( np . array ( math . sqrt ( abs ( d )))) for i in range ( len ( w2dis )): w2dis [ i ] = np . array ( w2dis [ i ]) return w2dis","title":"Wasserstein2Distance"},{"location":"evaluation/#src.desgld.evaluation.Wasserstein2Distance.W2_dist","text":"Class for: Wasserstein 2 distance in Bayesian Linear Regression Parameters: size_w ( int ) \u2013 the size of the network T ( int ) \u2013 the number of iterations avg_post ( list ) \u2013 the mean of the posterior distribution cov_post ( list ) \u2013 the covariance of the posterior distribution history_all ( list ) \u2013 contains the approximation from all the nodes beta_mean_all ( list ) \u2013 contains the mean of the approximation w2dis (list): contains the W2 distance of each agent and the mean of the approximation from all the ageents Source code in src/desgld/evaluation.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def W2_dist ( self ): \"\"\"Class for: Wasserstein 2 distance in Bayesian Linear Regression Args: size_w (int): the size of the network T (int): the number of iterations avg_post (list): the mean of the posterior distribution cov_post (list): the covariance of the posterior distribution history_all (list): contains the approximation from all the nodes beta_mean_all (list): contains the mean of the approximation from all the nodes Returns: w2dis (list): contains the W2 distance of each agent and the mean of the approximation from all the ageents \"\"\" w2dis = [] for i in range ( self . size_w ): temp = [] w2dis . append ( temp ) temp = [] w2dis . append ( temp ) \"\"\" W2 distance of each agent \"\"\" for i in range ( self . size_w ): for t in range ( self . T + 1 ): d = 0 avg_temp = [] avg_temp . append ( np . mean ( self . history_all [ t ][ i ][ 0 ])) avg_temp . append ( np . mean ( self . history_all [ t ][ i ][ 1 ])) avg_temp = np . array ( avg_temp ) cov_temp = np . cov ( self . history_all [ t ][ i ]) d = np . linalg . norm ( self . avg_post - avg_temp ) * np . linalg . norm ( self . avg_post - avg_temp ) d = d + np . trace ( self . cov_post + cov_temp - 2 * sqrtm ( np . dot ( np . dot ( sqrtm ( cov_temp ), self . cov_post ), sqrtm ( cov_temp ), ) ) ) w2dis [ i ] . append ( np . array ( math . sqrt ( abs ( d )))) \"\"\" W2 distance of the mean of agents \"\"\" for t in range ( self . T + 1 ): d = 0 avg_temp = [] avg_temp . append ( np . mean ( self . beta_mean_all [ t ][ 0 ])) avg_temp . append ( np . mean ( self . beta_mean_all [ t ][ 1 ])) avg_temp = np . array ( avg_temp ) cov_temp = np . cov ( self . beta_mean_all [ t ]) d = np . linalg . norm ( self . avg_post - avg_temp ) * np . linalg . norm ( self . avg_post - avg_temp ) d = d + np . trace ( self . cov_post + cov_temp - 2 * sqrtm ( np . dot ( np . dot ( sqrtm ( cov_temp ), self . cov_post ), sqrtm ( cov_temp ) ) ) ) w2dis [ self . size_w ] . append ( np . array ( math . sqrt ( abs ( d )))) for i in range ( len ( w2dis )): w2dis [ i ] = np . array ( w2dis [ i ]) return w2dis","title":"W2_dist"},{"location":"net/","text":"Network Structure This module provides a breief description of the decentralized network structure. NetworkArchitecture Decentralized network architecture Description This class is used to generate different network architectures. Here we add four different network architectures: fully connected, fully disconnected, circular network and star network. Source code in src/desgld/network.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class NetworkArchitecture : \"\"\"Decentralized network architecture Description: This class is used to generate different network architectures. Here we add four different network architectures: fully connected, fully disconnected, circular network and star network. Args: size_w (int): the size of the network Returns: Different network architectures \"\"\" def __init__ ( self , size_w ): self . size_w = size_w def fully_connected ( self ): \"\"\"Fully Connected Network Description: A fully connected network is a kind of network in which all nodes are connected to all other nodes. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Examples: >>> w=NetworkArchitecture(size_w=5) >>> w=w.fully_connected() >>> print(w) [[0.67811004 0.08047249 0.08047249 0.08047249 0.08047249] [0.08047249 0.67811004 0.08047249 0.08047249 0.08047249] [0.08047249 0.08047249 0.67811004 0.08047249 0.08047249] [0.08047249 0.08047249 0.08047249 0.67811004 0.08047249] [0.08047249 0.08047249 0.08047249 0.08047249 0.67811004]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] Returns: w (float): a fully connected network matrix \"\"\" adj_mat = np . ones (( self . size_w , self . size_w )) - np . eye ( self . size_w ) deg_mat = ( self . size_w - 1 ) * np . eye ( self . size_w ) laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w def circular_network ( self ): \"\"\"Circular Network Description: A circular network is a kind of network in which a particular node is connected to its left and right nodes only. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Returns: w (float): a circular network matrix Examples: >>> net = NetworkArchitecture(size_w=5) >>> w=net.circular_network() >>> print(w) [[0.72162653 0.13918674 0. 0. 0.13918674] [0.13918674 0.72162653 0.13918674 0. 0. ] [0. 0.13918674 0.72162653 0.13918674 0. ] [0. 0. 0.13918674 0.72162653 0.13918674] [0.13918674 0. 0. 0.13918674 0.72162653]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] \"\"\" adj_mat = np . zeros (( self . size_w , self . size_w )) for i in range ( self . size_w ): for j in range ( self . size_w ): if ( i + 1 ) == j : adj_mat [ i , j ] = 1 else : adj_mat [ i , j ] = adj_mat [ i , j ] adj_mat [ 0 , ( self . size_w - 1 )] = 1 adj_mat = adj_mat + np . transpose ( adj_mat ) deg_mat = 2 * np . eye ( self . size_w ) laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w def fully_disconnected ( self ): \"\"\"Completely Disconnected Network Description: A completely disconnected network is a kind of network in which all the nodes are disconnected from each other. Returns: w (float): a disconnected network matrix Examples: >>> net = NetworkArchitecture(size_w=6) >>> w=net.circular() >>> print(w) [[1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]] \"\"\" x = [[ 0 ] * self . size_w for _ in range ( self . size_w )] for i in range ( self . size_w ): for j in range ( self . size_w ): if i == j : x [ i ][ j ] = 1 return np . array ( x ) def star_network ( self ): \"\"\"Star-like Connected Network Description: A star-like network is a kind of network in which there is a central node and all other nodes are connected to the central node. However, the individual nodes are not connected to each other. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Returns: w (float): a star network matrix Examples: >>> net = NetworkArchitecture(size_w=5) >>> w=net.star_network() >>> print(w) [[0.71180893 0.07204777 0.07204777 0.07204777 0.07204777] [0.07204777 0.92795223 0. 0. 0. ] [0.07204777 0. 0.92795223 0. 0. ] [0.07204777 0. 0. 0.92795223 0. ] [0.07204777 0. 0. 0. 0.92795223]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] \"\"\" adj_mat = np . zeros (( self . size_w , self . size_w )) for i in range ( self . size_w ): for j in range ( self . size_w ): if i == 0 or j == 0 : adj_mat [ i , j ] = 1 adj_mat [ i , i ] = 0 else : adj_mat [ i , j ] = adj_mat [ i , j ] deg_mat = np . eye ( self . size_w ) deg_mat [ 0 , 0 ] = self . size_w - 1 laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w circular_network () Circular Network Description A circular network is a kind of network in which a particular node is connected to its left and right nodes only. Parameters: deg_mat(np.ndarray) \u2013 degree matrix of the graph Laplacian adj_mat(np.ndarray) \u2013 adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray) \u2013 graph Laplacian matrix eigenvalues ( ndarray ) \u2013 eigenvalues of the graph Laplacian max_eigenvalue ( float ) \u2013 maximum eigenvalue of the graph Laplacian reg_delta ( float ) \u2013 regular delta min_delta ( float ) \u2013 minimum delta for positive w Returns: w ( float ) \u2013 a circular network matrix Examples: >>> net = NetworkArchitecture ( size_w = 5 ) >>> w = net . circular_network () >>> print ( w ) [[0.72162653 0.13918674 0. 0. 0.13918674] [0.13918674 0.72162653 0.13918674 0. 0. ] [0. 0.13918674 0.72162653 0.13918674 0. ] [0. 0. 0.13918674 0.72162653 0.13918674] [0.13918674 0. 0. 0.13918674 0.72162653]] >>> print ( \"Row sums:\" , np . sum ( w , axis = 0 )) Row sums: [1. 1. 1. 1. 1.] >>> print ( \"Column sums:\" , np . sum ( w , axis = 1 )) Column sums: [1. 1. 1. 1. 1.] Source code in src/desgld/network.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def circular_network ( self ): \"\"\"Circular Network Description: A circular network is a kind of network in which a particular node is connected to its left and right nodes only. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Returns: w (float): a circular network matrix Examples: >>> net = NetworkArchitecture(size_w=5) >>> w=net.circular_network() >>> print(w) [[0.72162653 0.13918674 0. 0. 0.13918674] [0.13918674 0.72162653 0.13918674 0. 0. ] [0. 0.13918674 0.72162653 0.13918674 0. ] [0. 0. 0.13918674 0.72162653 0.13918674] [0.13918674 0. 0. 0.13918674 0.72162653]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] \"\"\" adj_mat = np . zeros (( self . size_w , self . size_w )) for i in range ( self . size_w ): for j in range ( self . size_w ): if ( i + 1 ) == j : adj_mat [ i , j ] = 1 else : adj_mat [ i , j ] = adj_mat [ i , j ] adj_mat [ 0 , ( self . size_w - 1 )] = 1 adj_mat = adj_mat + np . transpose ( adj_mat ) deg_mat = 2 * np . eye ( self . size_w ) laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w fully_connected () Fully Connected Network Description A fully connected network is a kind of network in which all nodes are connected to all other nodes. Parameters: deg_mat(np.ndarray) \u2013 degree matrix of the graph Laplacian adj_mat(np.ndarray) \u2013 adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray) \u2013 graph Laplacian matrix eigenvalues ( ndarray ) \u2013 eigenvalues of the graph Laplacian max_eigenvalue ( float ) \u2013 maximum eigenvalue of the graph Laplacian reg_delta ( float ) \u2013 regular delta min_delta ( float ) \u2013 minimum delta for positive w Examples: >>> w = NetworkArchitecture ( size_w = 5 ) >>> w = w . fully_connected () >>> print ( w ) [[0.67811004 0.08047249 0.08047249 0.08047249 0.08047249] [0.08047249 0.67811004 0.08047249 0.08047249 0.08047249] [0.08047249 0.08047249 0.67811004 0.08047249 0.08047249] [0.08047249 0.08047249 0.08047249 0.67811004 0.08047249] [0.08047249 0.08047249 0.08047249 0.08047249 0.67811004]] >>> print ( \"Row sums:\" , np . sum ( w , axis = 0 )) Row sums: [1. 1. 1. 1. 1.] >>> print ( \"Column sums:\" , np . sum ( w , axis = 1 )) Column sums: [1. 1. 1. 1. 1.] Source code in src/desgld/network.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def fully_connected ( self ): \"\"\"Fully Connected Network Description: A fully connected network is a kind of network in which all nodes are connected to all other nodes. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Examples: >>> w=NetworkArchitecture(size_w=5) >>> w=w.fully_connected() >>> print(w) [[0.67811004 0.08047249 0.08047249 0.08047249 0.08047249] [0.08047249 0.67811004 0.08047249 0.08047249 0.08047249] [0.08047249 0.08047249 0.67811004 0.08047249 0.08047249] [0.08047249 0.08047249 0.08047249 0.67811004 0.08047249] [0.08047249 0.08047249 0.08047249 0.08047249 0.67811004]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] Returns: w (float): a fully connected network matrix \"\"\" adj_mat = np . ones (( self . size_w , self . size_w )) - np . eye ( self . size_w ) deg_mat = ( self . size_w - 1 ) * np . eye ( self . size_w ) laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w fully_disconnected () Completely Disconnected Network Description A completely disconnected network is a kind of network in which all the nodes are disconnected from each other. Returns: w ( float ) \u2013 a disconnected network matrix Examples: >>> net = NetworkArchitecture ( size_w = 6 ) >>> w = net . circular () >>> print ( w ) [[1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]] Source code in src/desgld/network.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def fully_disconnected ( self ): \"\"\"Completely Disconnected Network Description: A completely disconnected network is a kind of network in which all the nodes are disconnected from each other. Returns: w (float): a disconnected network matrix Examples: >>> net = NetworkArchitecture(size_w=6) >>> w=net.circular() >>> print(w) [[1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]] \"\"\" x = [[ 0 ] * self . size_w for _ in range ( self . size_w )] for i in range ( self . size_w ): for j in range ( self . size_w ): if i == j : x [ i ][ j ] = 1 return np . array ( x ) star_network () Star-like Connected Network Description A star-like network is a kind of network in which there is a central node and all other nodes are connected to the central node. However, the individual nodes are not connected to each other. Parameters: deg_mat(np.ndarray) \u2013 degree matrix of the graph Laplacian adj_mat(np.ndarray) \u2013 adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray) \u2013 graph Laplacian matrix eigenvalues ( ndarray ) \u2013 eigenvalues of the graph Laplacian max_eigenvalue ( float ) \u2013 maximum eigenvalue of the graph Laplacian reg_delta ( float ) \u2013 regular delta min_delta ( float ) \u2013 minimum delta for positive w Returns: w ( float ) \u2013 a star network matrix Examples: >>> net = NetworkArchitecture ( size_w = 5 ) >>> w = net . star_network () >>> print ( w ) [[0.71180893 0.07204777 0.07204777 0.07204777 0.07204777] [0.07204777 0.92795223 0. 0. 0. ] [0.07204777 0. 0.92795223 0. 0. ] [0.07204777 0. 0. 0.92795223 0. ] [0.07204777 0. 0. 0. 0.92795223]] >>> print ( \"Row sums:\" , np . sum ( w , axis = 0 )) Row sums: [1. 1. 1. 1. 1.] >>> print ( \"Column sums:\" , np . sum ( w , axis = 1 )) Column sums: [1. 1. 1. 1. 1.] Source code in src/desgld/network.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def star_network ( self ): \"\"\"Star-like Connected Network Description: A star-like network is a kind of network in which there is a central node and all other nodes are connected to the central node. However, the individual nodes are not connected to each other. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Returns: w (float): a star network matrix Examples: >>> net = NetworkArchitecture(size_w=5) >>> w=net.star_network() >>> print(w) [[0.71180893 0.07204777 0.07204777 0.07204777 0.07204777] [0.07204777 0.92795223 0. 0. 0. ] [0.07204777 0. 0.92795223 0. 0. ] [0.07204777 0. 0. 0.92795223 0. ] [0.07204777 0. 0. 0. 0.92795223]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] \"\"\" adj_mat = np . zeros (( self . size_w , self . size_w )) for i in range ( self . size_w ): for j in range ( self . size_w ): if i == 0 or j == 0 : adj_mat [ i , j ] = 1 adj_mat [ i , i ] = 0 else : adj_mat [ i , j ] = adj_mat [ i , j ] deg_mat = np . eye ( self . size_w ) deg_mat [ 0 , 0 ] = self . size_w - 1 laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w","title":"Network Structure"},{"location":"net/#network-structure","text":"This module provides a breief description of the decentralized network structure.","title":"Network Structure"},{"location":"net/#src.desgld.network.NetworkArchitecture","text":"Decentralized network architecture Description This class is used to generate different network architectures. Here we add four different network architectures: fully connected, fully disconnected, circular network and star network. Source code in src/desgld/network.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class NetworkArchitecture : \"\"\"Decentralized network architecture Description: This class is used to generate different network architectures. Here we add four different network architectures: fully connected, fully disconnected, circular network and star network. Args: size_w (int): the size of the network Returns: Different network architectures \"\"\" def __init__ ( self , size_w ): self . size_w = size_w def fully_connected ( self ): \"\"\"Fully Connected Network Description: A fully connected network is a kind of network in which all nodes are connected to all other nodes. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Examples: >>> w=NetworkArchitecture(size_w=5) >>> w=w.fully_connected() >>> print(w) [[0.67811004 0.08047249 0.08047249 0.08047249 0.08047249] [0.08047249 0.67811004 0.08047249 0.08047249 0.08047249] [0.08047249 0.08047249 0.67811004 0.08047249 0.08047249] [0.08047249 0.08047249 0.08047249 0.67811004 0.08047249] [0.08047249 0.08047249 0.08047249 0.08047249 0.67811004]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] Returns: w (float): a fully connected network matrix \"\"\" adj_mat = np . ones (( self . size_w , self . size_w )) - np . eye ( self . size_w ) deg_mat = ( self . size_w - 1 ) * np . eye ( self . size_w ) laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w def circular_network ( self ): \"\"\"Circular Network Description: A circular network is a kind of network in which a particular node is connected to its left and right nodes only. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Returns: w (float): a circular network matrix Examples: >>> net = NetworkArchitecture(size_w=5) >>> w=net.circular_network() >>> print(w) [[0.72162653 0.13918674 0. 0. 0.13918674] [0.13918674 0.72162653 0.13918674 0. 0. ] [0. 0.13918674 0.72162653 0.13918674 0. ] [0. 0. 0.13918674 0.72162653 0.13918674] [0.13918674 0. 0. 0.13918674 0.72162653]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] \"\"\" adj_mat = np . zeros (( self . size_w , self . size_w )) for i in range ( self . size_w ): for j in range ( self . size_w ): if ( i + 1 ) == j : adj_mat [ i , j ] = 1 else : adj_mat [ i , j ] = adj_mat [ i , j ] adj_mat [ 0 , ( self . size_w - 1 )] = 1 adj_mat = adj_mat + np . transpose ( adj_mat ) deg_mat = 2 * np . eye ( self . size_w ) laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w def fully_disconnected ( self ): \"\"\"Completely Disconnected Network Description: A completely disconnected network is a kind of network in which all the nodes are disconnected from each other. Returns: w (float): a disconnected network matrix Examples: >>> net = NetworkArchitecture(size_w=6) >>> w=net.circular() >>> print(w) [[1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]] \"\"\" x = [[ 0 ] * self . size_w for _ in range ( self . size_w )] for i in range ( self . size_w ): for j in range ( self . size_w ): if i == j : x [ i ][ j ] = 1 return np . array ( x ) def star_network ( self ): \"\"\"Star-like Connected Network Description: A star-like network is a kind of network in which there is a central node and all other nodes are connected to the central node. However, the individual nodes are not connected to each other. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Returns: w (float): a star network matrix Examples: >>> net = NetworkArchitecture(size_w=5) >>> w=net.star_network() >>> print(w) [[0.71180893 0.07204777 0.07204777 0.07204777 0.07204777] [0.07204777 0.92795223 0. 0. 0. ] [0.07204777 0. 0.92795223 0. 0. ] [0.07204777 0. 0. 0.92795223 0. ] [0.07204777 0. 0. 0. 0.92795223]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] \"\"\" adj_mat = np . zeros (( self . size_w , self . size_w )) for i in range ( self . size_w ): for j in range ( self . size_w ): if i == 0 or j == 0 : adj_mat [ i , j ] = 1 adj_mat [ i , i ] = 0 else : adj_mat [ i , j ] = adj_mat [ i , j ] deg_mat = np . eye ( self . size_w ) deg_mat [ 0 , 0 ] = self . size_w - 1 laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w","title":"NetworkArchitecture"},{"location":"net/#src.desgld.network.NetworkArchitecture.circular_network","text":"Circular Network Description A circular network is a kind of network in which a particular node is connected to its left and right nodes only. Parameters: deg_mat(np.ndarray) \u2013 degree matrix of the graph Laplacian adj_mat(np.ndarray) \u2013 adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray) \u2013 graph Laplacian matrix eigenvalues ( ndarray ) \u2013 eigenvalues of the graph Laplacian max_eigenvalue ( float ) \u2013 maximum eigenvalue of the graph Laplacian reg_delta ( float ) \u2013 regular delta min_delta ( float ) \u2013 minimum delta for positive w Returns: w ( float ) \u2013 a circular network matrix Examples: >>> net = NetworkArchitecture ( size_w = 5 ) >>> w = net . circular_network () >>> print ( w ) [[0.72162653 0.13918674 0. 0. 0.13918674] [0.13918674 0.72162653 0.13918674 0. 0. ] [0. 0.13918674 0.72162653 0.13918674 0. ] [0. 0. 0.13918674 0.72162653 0.13918674] [0.13918674 0. 0. 0.13918674 0.72162653]] >>> print ( \"Row sums:\" , np . sum ( w , axis = 0 )) Row sums: [1. 1. 1. 1. 1.] >>> print ( \"Column sums:\" , np . sum ( w , axis = 1 )) Column sums: [1. 1. 1. 1. 1.] Source code in src/desgld/network.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def circular_network ( self ): \"\"\"Circular Network Description: A circular network is a kind of network in which a particular node is connected to its left and right nodes only. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Returns: w (float): a circular network matrix Examples: >>> net = NetworkArchitecture(size_w=5) >>> w=net.circular_network() >>> print(w) [[0.72162653 0.13918674 0. 0. 0.13918674] [0.13918674 0.72162653 0.13918674 0. 0. ] [0. 0.13918674 0.72162653 0.13918674 0. ] [0. 0. 0.13918674 0.72162653 0.13918674] [0.13918674 0. 0. 0.13918674 0.72162653]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] \"\"\" adj_mat = np . zeros (( self . size_w , self . size_w )) for i in range ( self . size_w ): for j in range ( self . size_w ): if ( i + 1 ) == j : adj_mat [ i , j ] = 1 else : adj_mat [ i , j ] = adj_mat [ i , j ] adj_mat [ 0 , ( self . size_w - 1 )] = 1 adj_mat = adj_mat + np . transpose ( adj_mat ) deg_mat = 2 * np . eye ( self . size_w ) laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w","title":"circular_network"},{"location":"net/#src.desgld.network.NetworkArchitecture.fully_connected","text":"Fully Connected Network Description A fully connected network is a kind of network in which all nodes are connected to all other nodes. Parameters: deg_mat(np.ndarray) \u2013 degree matrix of the graph Laplacian adj_mat(np.ndarray) \u2013 adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray) \u2013 graph Laplacian matrix eigenvalues ( ndarray ) \u2013 eigenvalues of the graph Laplacian max_eigenvalue ( float ) \u2013 maximum eigenvalue of the graph Laplacian reg_delta ( float ) \u2013 regular delta min_delta ( float ) \u2013 minimum delta for positive w Examples: >>> w = NetworkArchitecture ( size_w = 5 ) >>> w = w . fully_connected () >>> print ( w ) [[0.67811004 0.08047249 0.08047249 0.08047249 0.08047249] [0.08047249 0.67811004 0.08047249 0.08047249 0.08047249] [0.08047249 0.08047249 0.67811004 0.08047249 0.08047249] [0.08047249 0.08047249 0.08047249 0.67811004 0.08047249] [0.08047249 0.08047249 0.08047249 0.08047249 0.67811004]] >>> print ( \"Row sums:\" , np . sum ( w , axis = 0 )) Row sums: [1. 1. 1. 1. 1.] >>> print ( \"Column sums:\" , np . sum ( w , axis = 1 )) Column sums: [1. 1. 1. 1. 1.] Source code in src/desgld/network.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def fully_connected ( self ): \"\"\"Fully Connected Network Description: A fully connected network is a kind of network in which all nodes are connected to all other nodes. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Examples: >>> w=NetworkArchitecture(size_w=5) >>> w=w.fully_connected() >>> print(w) [[0.67811004 0.08047249 0.08047249 0.08047249 0.08047249] [0.08047249 0.67811004 0.08047249 0.08047249 0.08047249] [0.08047249 0.08047249 0.67811004 0.08047249 0.08047249] [0.08047249 0.08047249 0.08047249 0.67811004 0.08047249] [0.08047249 0.08047249 0.08047249 0.08047249 0.67811004]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] Returns: w (float): a fully connected network matrix \"\"\" adj_mat = np . ones (( self . size_w , self . size_w )) - np . eye ( self . size_w ) deg_mat = ( self . size_w - 1 ) * np . eye ( self . size_w ) laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w","title":"fully_connected"},{"location":"net/#src.desgld.network.NetworkArchitecture.fully_disconnected","text":"Completely Disconnected Network Description A completely disconnected network is a kind of network in which all the nodes are disconnected from each other. Returns: w ( float ) \u2013 a disconnected network matrix Examples: >>> net = NetworkArchitecture ( size_w = 6 ) >>> w = net . circular () >>> print ( w ) [[1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]] Source code in src/desgld/network.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def fully_disconnected ( self ): \"\"\"Completely Disconnected Network Description: A completely disconnected network is a kind of network in which all the nodes are disconnected from each other. Returns: w (float): a disconnected network matrix Examples: >>> net = NetworkArchitecture(size_w=6) >>> w=net.circular() >>> print(w) [[1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]] \"\"\" x = [[ 0 ] * self . size_w for _ in range ( self . size_w )] for i in range ( self . size_w ): for j in range ( self . size_w ): if i == j : x [ i ][ j ] = 1 return np . array ( x )","title":"fully_disconnected"},{"location":"net/#src.desgld.network.NetworkArchitecture.star_network","text":"Star-like Connected Network Description A star-like network is a kind of network in which there is a central node and all other nodes are connected to the central node. However, the individual nodes are not connected to each other. Parameters: deg_mat(np.ndarray) \u2013 degree matrix of the graph Laplacian adj_mat(np.ndarray) \u2013 adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray) \u2013 graph Laplacian matrix eigenvalues ( ndarray ) \u2013 eigenvalues of the graph Laplacian max_eigenvalue ( float ) \u2013 maximum eigenvalue of the graph Laplacian reg_delta ( float ) \u2013 regular delta min_delta ( float ) \u2013 minimum delta for positive w Returns: w ( float ) \u2013 a star network matrix Examples: >>> net = NetworkArchitecture ( size_w = 5 ) >>> w = net . star_network () >>> print ( w ) [[0.71180893 0.07204777 0.07204777 0.07204777 0.07204777] [0.07204777 0.92795223 0. 0. 0. ] [0.07204777 0. 0.92795223 0. 0. ] [0.07204777 0. 0. 0.92795223 0. ] [0.07204777 0. 0. 0. 0.92795223]] >>> print ( \"Row sums:\" , np . sum ( w , axis = 0 )) Row sums: [1. 1. 1. 1. 1.] >>> print ( \"Column sums:\" , np . sum ( w , axis = 1 )) Column sums: [1. 1. 1. 1. 1.] Source code in src/desgld/network.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def star_network ( self ): \"\"\"Star-like Connected Network Description: A star-like network is a kind of network in which there is a central node and all other nodes are connected to the central node. However, the individual nodes are not connected to each other. Args: deg_mat(np.ndarray): degree matrix of the graph Laplacian adj_mat(np.ndarray): adjacency matrix of the graph Laplacian laplacian_matrix(np.ndarray): graph Laplacian matrix eigenvalues (np.ndarray): eigenvalues of the graph Laplacian max_eigenvalue (float): maximum eigenvalue of the graph Laplacian reg_delta (float): regular delta min_delta (float): minimum delta for positive w Returns: w (float): a star network matrix Examples: >>> net = NetworkArchitecture(size_w=5) >>> w=net.star_network() >>> print(w) [[0.71180893 0.07204777 0.07204777 0.07204777 0.07204777] [0.07204777 0.92795223 0. 0. 0. ] [0.07204777 0. 0.92795223 0. 0. ] [0.07204777 0. 0. 0.92795223 0. ] [0.07204777 0. 0. 0. 0.92795223]] >>> print(\"Row sums:\", np.sum(w, axis=0)) Row sums: [1. 1. 1. 1. 1.] >>> print(\"Column sums:\", np.sum(w, axis=1)) Column sums: [1. 1. 1. 1. 1.] \"\"\" adj_mat = np . zeros (( self . size_w , self . size_w )) for i in range ( self . size_w ): for j in range ( self . size_w ): if i == 0 or j == 0 : adj_mat [ i , j ] = 1 adj_mat [ i , i ] = 0 else : adj_mat [ i , j ] = adj_mat [ i , j ] deg_mat = np . eye ( self . size_w ) deg_mat [ 0 , 0 ] = self . size_w - 1 laplacian_matrix = deg_mat - adj_mat eigenvalues = np . linalg . eigvals ( laplacian_matrix ) max_eigenvalue = np . max ( np . real ( eigenvalues )) reg_delta = random . uniform ( 0 , ( 2 / max_eigenvalue )) min_delta = np . min ( laplacian_matrix ) / np . max ( deg_mat ) delta = max ( reg_delta , min_delta ) w = np . eye ( self . size_w ) - delta * laplacian_matrix return w","title":"star_network"},{"location":"use/","text":"Usage Example Bellow is an example of how to use the package `desgld`. Just like any other package you usually install, you can install the `desgld` package using the following command either in terminal or command line pip install git+ssh://git@github.com/mrislambd/desgld_package.git or simply run pip install desgld Demo Example Here this is a complete example of how to use the package desgld . import matplotlib.pyplot as plt import matplotlib as mpl import numpy as np from tqdm import tqdm from sklearn.model_selection import train_test_split from desgld import DeSGLD from desgld import NetworkArchitecture from desgld import ClassificationAccuracy from desgld import Wasserstein2Distance Bayesian Logistic Regression with Synthetic Data Parameters size_w=6; N=100; sigma=1; eta=0.0005; T=20; dim=3; b=32; lam=10; total_data=1000; hv=np.linspace(0.001,0.5,5) Data x = [] np . random . seed ( 10 ) for i in range ( total_data ) : x . append ( [ -20 + (20 + 20) * np.random.normal(), -10 + np.random.normal() ] ) np . random . seed ( 11 ) y = [ 1 / (1 + np.exp(-item[0 ] * 1 - 1 * item [ 1 ] + 10 )) for item in x ] for i in range ( len ( y )) : temp = np . random . uniform ( 0 , 1 ) if temp <= y [ i ] : y [ i ] = 1 else : y [ i ] = 0 x_all = np . array ( x ) y_all = np . array ( y ) x_all = x y_all = y x_all = np . insert ( x_all , 0 , 1 , axis = 1 ) x = x_all ''' Data splitting ''' X_train1 , x_trainRemain , y_train1 , y_trainRemain = train_test_split ( x , y , test_size = 0.83333 , random_state = 42 ) X_train2 , x_trainRemain , y_train2 , y_trainRemain = train_test_split ( x_trainRemain , y_trainRemain , test_size = 0.8 , random_state = 42 ) X_train3 , x_trainRemain , y_train3 , y_trainRemain = train_test_split ( x_trainRemain , y_trainRemain , test_size = 0.75 , random_state = 42 ) X_train4 , x_trainRemain , y_train4 , y_trainRemain = train_test_split ( x_trainRemain , y_trainRemain , test_size = 0.66666666 , random_state = 42 ) X_train5 , X_train6 , y_train5 , y_train6 = train_test_split ( x_trainRemain , y_trainRemain , test_size = 0.5 , random_state = 42 ) x = [ X_train1, X_train2, X_train3, X_train4, X_train5, X_train6 ] y = [ y_train1, y_train2, y_train3, y_train4, y_train5, y_train6 ] Vanila DeSGLD approximation net=NetworkArchitecture(size_w) wf=net.fully_connected() wc=net.circular_network() wd=net.fully_disconnected() nets=np.array([wf,wc,wd]) Run the Vanila DeSGLD algorithm for all three networks three_net_combined = [] for i in tqdm ( range ( len ( nets ))) : method = DeSGLD ( size_w , N , sigma , eta , T , dim , b , lam , x , y , nets [ i ] , hv , 'logistic' ) vdesgld , _ = method . vanila_desgld () three_net_combined . append ( vdesgld ) Compute the accuracy desgld_acc=[] desgld_std=[] for desgld in tqdm(three_net_combined): desgld_accuracy=ClassificationAccuracy(x_all,y_all,desgld,T) d_acc,d_std=desgld_accuracy.compute_accuracy() desgld_acc.append(d_acc) desgld_std.append(d_std) desgld_acc=np.array(desgld_acc) desgld_std=np.array(desgld_std) Next we plot the accuracy of the classification from the vanila DeSGLD fig , axs = plt . subplots ( 1 , len ( desgld_acc ), figsize = ( 26.4 , 6.6 )) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles =[ \"Fully Connected\",\"Circular\",\"Disconnected\" ] for i , ( d_acc , d_std , title ) in enumerate ( zip ( desgld_acc , desgld_std , titles )) : axs [ i ] . plot ( d_acc , 'b-' , linewidth = 3 ) axs [ i ] . fill_between ( index , d_acc + d_std , d_acc - d_std , alpha = 0.5 ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . legend ( [ 'Mean of accuracy', r'Accuracy $\\pm$ Std' ] , loc = 'lower right' , fontsize = 24 ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 20 ) axs [ i ] . set_ylabel ( 'Accuracy' , fontsize = 24 ) axs [ i ] . set_ylim ( 0 , 1 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . grid () plt . tight_layout () plt . show () The plot EXTRA-DeSGLD Approximation three_net_combined = [] for i in tqdm ( range ( len ( nets ))) : method = DeSGLD ( size_w , N , sigma , eta , T , dim , b , lam , x , y , nets [ i ] , hv , 'logistic' ) edesgld , _ = method . extra_desgld () three_net_combined . append ( edesgld ) Accuracy Calculations extradesgld_acc = [] extradesgld_std = [] for i in tqdm ( range ( len ( nets ))) : exacc = [] exstd = [] for j in range ( len ( hv )) : extradesgld_accuracy = ClassificationAccuracy ( x_all , y_all , three_net_combined [ i ][ j ] , T ) acc , std = extradesgld_accuracy . compute_accuracy () exacc . append ( acc ) exstd . append ( std ) extradesgld_acc . append ( exacc ) extradesgld_std . append ( exstd ) extradesgld_acc = np . array ( extradesgld_acc ) extradesgld_std = np . array ( extradesgld_std ) Plot of the EXTRA DE-SGLD for each of the h values fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 8.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles =[ 'Fully Connected','Circular','Disconnected' ] for j , title in enumerate ( titles ) : for i , h_value in enumerate ( hv ) : axs [ j ] . plot ( index , extradesgld_acc [ j ][ i ] , label = f 'h = {h_value:.3f}' ) axs [ j ] . set_title ( title , fontsize = 24 ) axs [ j ] . legend ( loc = 'lower right' ) axs [ j ] . set_xlabel ( r 'Iterations $k$' , fontsize = 20 ) axs [ j ] . set_ylabel ( 'Accuracy' , fontsize = 24 ) axs [ j ] . set_ylim ( 0 , 1 ) axs [ j ] . tick_params ( labelsize = 24 ) axs [ j ] . grid () plt . tight_layout () plt . show () Find the Maximum Accuracy for the optimal h values max_acc_arrays = [] max_acc_indices = [] for i in range ( len ( nets )) : max_acc_array = extradesgld_acc [ i ][ np.argmax(np.max(extradesgld_acc[i ] , axis = 1 )) ] max_acc_index = np . argmax ( np . max ( extradesgld_acc [ i ] , axis = 1 )) max_acc_arrays . append ( max_acc_array ) max_acc_indices . append ( max_acc_index ) max_acc_arrays = np . array ( max_acc_arrays ) max_acc_indices = np . array ( max_acc_indices ) max_hv_values = hv [ max_acc_indices ] max_acc_stds = np . zeros (( len ( nets ), ( T + 1 ))) for i in range ( len ( nets )) : max_acc_stds [ i ] = extradesgld_std [ i, max_acc_indices[i ] ] Next plot the maximum accuracy for the optimal h fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 6.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles =[ 'Fully Connected','Circular','Disconnected' ] for i , title in enumerate ( titles ) : axs [ i ] . plot ( max_acc_arrays [ i ] , 'r-' , linewidth = 3 , label = f 'h = {max_hv_values[i]:.3f}' ) axs [ i ] . fill_between ( index , max_acc_arrays [ i ]+ max_acc_stds [ i ] , max_acc_arrays [ i ]- max_acc_stds [ i ] , alpha = 0.25 , color = 'red' ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . legend ( loc = 'lower right' ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( 'Accuracy' , fontsize = 24 ) axs [ i ] . set_ylim ( 0 , 1 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . grid () plt . tight_layout () plt . show () Now we can compare the approximations from both of the Vanila and Extra DeSGLD algorithms fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 6.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles = [ 'Fully Connected', 'Circular', 'Disconnected' ] for i , title in enumerate ( titles ) : axs [ i ] . plot ( desgld_acc [ i ] , 'b-' , linewidth = 3 , label = f 'Vanila DESGLD' ) axs [ i ] . fill_between ( index , desgld_acc [ i ] + desgld_std [ i ] , desgld_acc [ i ] - desgld_std [ i ] , alpha = 0.25 , color = 'blue' ) axs [ i ] . plot ( max_acc_arrays [ i ] , 'r-' , linewidth = 3 , label = f 'EXTRA DESGLD, h = {max_hv_values[i]:.3f}' ) axs [ i ] . fill_between ( index , max_acc_arrays [ i ] + max_acc_stds [ i ] , max_acc_arrays [ i ] - max_acc_stds [ i ] , alpha = 0.25 , color = 'red' ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . legend ( loc = 'lower right' ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( 'Accuracy' , fontsize = 24 ) axs [ i ] . set_ylim ( 0 , 1 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . grid () plt . tight_layout () plt . show () Bayesian Linear Regression with Synthetic Data Parameters size_w=20; N=20; dim=2; sigma=np.eye(dim); sigma_sample=1; eta=0.009 ; T=50; lam=10; b=50; hv=np.linspace(0.001,0.5,5); Data Generation x=[] for i in range(1000): x.append([np.random.random()*1]) y=[item[0]*3-0.5+np.random.random() for item in x] x_all=np.array(x) y_all=np.array(y) x_all=np.insert(x_all, 0, 1, axis=1) cov_pri=lam*sigma avg_post=np.dot( np.linalg.inv(np.linalg.inv(cov_pri)+np.dot(np.transpose(x_all),x_all)/(sigma_sample**2)), (np.dot(np.transpose(x_all),y_all)/(sigma_sample**2)) ) cov_post=np.linalg.inv(np.linalg.inv(cov_pri)+np.dot(np.transpose(x_all),x_all)/(sigma_sample**2)) x=np.split(x_all,100) y=np.split(y_all,100) Vanila DeSGLD Approximation net=NetworkArchitecture(size_w) wf=net.fully_connected() wc=net.circular_network() wd=net.fully_disconnected() nets=np.array([wf,wc,wd]) Run the Vanila DeSGLD algorithm for all three networks three_net_combined_agents = [] three_net_combined_magents = [] for i in tqdm ( range ( len ( nets ))) : method = DeSGLD ( size_w , N , sigma , eta , T , dim , b , lam , x , y , nets [ i ] , hv , 'linear' ) desgld_agents , desgld_magents = method . vanila_desgld () three_net_combined_agents . append ( desgld_agents ) three_net_combined_magents . append ( desgld_magents ) Compute the w_2 distance desgld_three_net_combined_dist=[] for dis_agent,dis_magent in tqdm(zip(three_net_combined_agents,three_net_combined_magents)): distance=Wasserstein2Distance(size_w,T,avg_post,cov_post,dis_agent,dis_magent) dis=distance.W2_dist() desgld_three_net_combined_dist.append(dis) Next plot the distances fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 6.6 )) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles =[ \"Fully Connected: DESGLD\",\"Circular: DESGLD\",\"Disconnected: DESGLD\" ] for i in range ( len ( desgld_three_net_combined_dist )) : axs [ i ] . plot ( index , desgld_three_net_combined_dist [ i ][ 0 ] , linewidth = 3 , label = r 'Agent 1 $x_1^{(k)}$' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ 1 ] , linewidth = 3 , label = r 'Agent 2 $x_2^{(k)}$' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ 2 ] , linewidth = 3 , label = r 'Agent 3 $x_3^{(k)}$' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ 3 ] , linewidth = 3 , label = r 'Agent 4 $x_4^{(k)}$' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ -1 ] , linewidth = 3 , label = r 'Mean of Agents $\\bar{x}^{(k)}$' ) axs [ i ] . set_title ( titles [ i ] , fontsize = 24 ) axs [ i ] . legend ( loc = 'upper right' , fontsize = 17 ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( r '$\\mathcal{W}_2$ Distance ' , fontsize = 24 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . grid () plt . tight_layout () plt . show () Here is the plot EXTRA DeSGLD Approximation Run the EXTRA DeSGLD algorithm for all three networks three_net_combined_agents = [] three_net_combined_magents = [] for i in tqdm ( range ( len ( nets ))) : method = DeSGLD ( size_w , N , sigma , eta , T , dim , b , lam , x , y , nets [ i ] , hv , 'linear' ) edesgld_agents , edesgld_magents = method . extra_desgld () three_net_combined_agents . append ( edesgld_agents ) three_net_combined_magents . append ( edesgld_magents ) Compute the w_2 Distances extradesgld_three_net_combined_dist = [] for i in tqdm ( range ( len ( nets ))) : ex = [] for j in range ( len ( hv )) : extradistance = Wasserstein2Distance ( size_w , T , avg_post , cov_post , three_net_combined_agents [ i ][ j ] , three_net_combined_magents [ i ][ j ] ) ex . append ( extradistance . W2_dist ()) extradesgld_three_net_combined_dist . append ( ex ) extradesgld_three_net_combined_dist = np . array ( extradesgld_three_net_combined_dist ) Work with the w_2 distances of mean agents mean_dist = np.empty((len(nets), len(hv), T+1)) for i in tqdm(range(len(nets))): for j in range(len(hv)): mean_dist[i, j, :] = extradesgld_three_net_combined_dist[i, j, -1, :] Plot the w_2 distance of mean agents for each h values fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 8.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ]= 24 titles =[ 'Fully Connected','Circular', 'Disconnected' ] for j , title in enumerate ( titles ) : for i , h_value in enumerate ( hv ) : axs [ j ] . plot ( mean_dist [ j ][ i ] , linewidth = 3 , label = f 'h={h_value:.3f}' ) axs [ j ] . set_title ( title , fontsize = 24 ) axs [ j ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ j ] . set_ylabel ( r '$\\mathcal{W}_2$ Dist. of mean agents' , fontsize = 24 ) axs [ j ] . tick_params ( labelsize = 24 ) axs [ j ] . legend ( loc = 'upper right' ) axs [ j ] . grid () plt . tight_layout () plt . show () Find the minimum distance for optimal h values min_dist_arrays = [] min_dist_indices = [] for i in range ( len ( nets )) : min_dist_array = mean_dist [ i ][ np.argmin(np.min(mean_dist[i ] , axis = 1 )) ] min_dist_index = np . argmin ( np . min ( mean_dist [ i ] , axis = 1 )) min_dist_arrays . append ( min_dist_array ) min_dist_indices . append ( min_dist_index ) min_dist_arrays = np . array ( min_dist_arrays ) min_dist_indices = np . array ( min_dist_indices ) min_hv_values = hv [ min_dist_indices ] min_hv_values Plot the Optimal EXTRA DeSGLD Mean Distance for the optimal h value fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 8.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ]= 24 titles =[ 'Fully Connected','Circular', 'Disconnected' ] for i , title in enumerate ( titles ) : axs [ i ] . plot ( min_dist_arrays [ i ] , 'r-' , linewidth = 3 , label = f 'h={min_hv_values[i]:.3f}' ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( r '$\\mathcal{W}_2$ Dist. of mean agents' , fontsize = 24 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . legend ( loc = 'upper right' ) axs [ i ] . grid () plt . tight_layout () plt . show () Finally, we can compare the performances from both of the algorithms fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 8.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ]= 24 titles =[ 'Fully Connected','Circular', 'Disconnected' ] for i , title in enumerate ( titles ) : axs [ i ] . plot ( min_dist_arrays [ i ] , 'r-' , linewidth = 3 , label = f 'EXTRA DESGLD for h={min_hv_values[i]:.3f}' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ -1 ] , linewidth = 3 , label = 'Vanila DESGLD' ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( r '$\\mathcal{W}_2$ Dist. of mean agents' , fontsize = 24 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . legend ( loc = 'upper right' ) axs [ i ] . grid () plt . tight_layout () plt . show () And, here is the plot","title":"Usage"},{"location":"use/#usage-example","text":"Bellow is an example of how to use the package `desgld`. Just like any other package you usually install, you can install the `desgld` package using the following command either in terminal or command line pip install git+ssh://git@github.com/mrislambd/desgld_package.git or simply run pip install desgld","title":"Usage Example"},{"location":"use/#demo-example","text":"Here this is a complete example of how to use the package desgld . import matplotlib.pyplot as plt import matplotlib as mpl import numpy as np from tqdm import tqdm from sklearn.model_selection import train_test_split from desgld import DeSGLD from desgld import NetworkArchitecture from desgld import ClassificationAccuracy from desgld import Wasserstein2Distance","title":"Demo Example"},{"location":"use/#bayesian-logistic-regression-with-synthetic-data","text":"Parameters size_w=6; N=100; sigma=1; eta=0.0005; T=20; dim=3; b=32; lam=10; total_data=1000; hv=np.linspace(0.001,0.5,5) Data x = [] np . random . seed ( 10 ) for i in range ( total_data ) : x . append ( [ -20 + (20 + 20) * np.random.normal(), -10 + np.random.normal() ] ) np . random . seed ( 11 ) y = [ 1 / (1 + np.exp(-item[0 ] * 1 - 1 * item [ 1 ] + 10 )) for item in x ] for i in range ( len ( y )) : temp = np . random . uniform ( 0 , 1 ) if temp <= y [ i ] : y [ i ] = 1 else : y [ i ] = 0 x_all = np . array ( x ) y_all = np . array ( y ) x_all = x y_all = y x_all = np . insert ( x_all , 0 , 1 , axis = 1 ) x = x_all ''' Data splitting ''' X_train1 , x_trainRemain , y_train1 , y_trainRemain = train_test_split ( x , y , test_size = 0.83333 , random_state = 42 ) X_train2 , x_trainRemain , y_train2 , y_trainRemain = train_test_split ( x_trainRemain , y_trainRemain , test_size = 0.8 , random_state = 42 ) X_train3 , x_trainRemain , y_train3 , y_trainRemain = train_test_split ( x_trainRemain , y_trainRemain , test_size = 0.75 , random_state = 42 ) X_train4 , x_trainRemain , y_train4 , y_trainRemain = train_test_split ( x_trainRemain , y_trainRemain , test_size = 0.66666666 , random_state = 42 ) X_train5 , X_train6 , y_train5 , y_train6 = train_test_split ( x_trainRemain , y_trainRemain , test_size = 0.5 , random_state = 42 ) x = [ X_train1, X_train2, X_train3, X_train4, X_train5, X_train6 ] y = [ y_train1, y_train2, y_train3, y_train4, y_train5, y_train6 ] Vanila DeSGLD approximation net=NetworkArchitecture(size_w) wf=net.fully_connected() wc=net.circular_network() wd=net.fully_disconnected() nets=np.array([wf,wc,wd]) Run the Vanila DeSGLD algorithm for all three networks three_net_combined = [] for i in tqdm ( range ( len ( nets ))) : method = DeSGLD ( size_w , N , sigma , eta , T , dim , b , lam , x , y , nets [ i ] , hv , 'logistic' ) vdesgld , _ = method . vanila_desgld () three_net_combined . append ( vdesgld ) Compute the accuracy desgld_acc=[] desgld_std=[] for desgld in tqdm(three_net_combined): desgld_accuracy=ClassificationAccuracy(x_all,y_all,desgld,T) d_acc,d_std=desgld_accuracy.compute_accuracy() desgld_acc.append(d_acc) desgld_std.append(d_std) desgld_acc=np.array(desgld_acc) desgld_std=np.array(desgld_std) Next we plot the accuracy of the classification from the vanila DeSGLD fig , axs = plt . subplots ( 1 , len ( desgld_acc ), figsize = ( 26.4 , 6.6 )) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles =[ \"Fully Connected\",\"Circular\",\"Disconnected\" ] for i , ( d_acc , d_std , title ) in enumerate ( zip ( desgld_acc , desgld_std , titles )) : axs [ i ] . plot ( d_acc , 'b-' , linewidth = 3 ) axs [ i ] . fill_between ( index , d_acc + d_std , d_acc - d_std , alpha = 0.5 ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . legend ( [ 'Mean of accuracy', r'Accuracy $\\pm$ Std' ] , loc = 'lower right' , fontsize = 24 ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 20 ) axs [ i ] . set_ylabel ( 'Accuracy' , fontsize = 24 ) axs [ i ] . set_ylim ( 0 , 1 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . grid () plt . tight_layout () plt . show () The plot EXTRA-DeSGLD Approximation three_net_combined = [] for i in tqdm ( range ( len ( nets ))) : method = DeSGLD ( size_w , N , sigma , eta , T , dim , b , lam , x , y , nets [ i ] , hv , 'logistic' ) edesgld , _ = method . extra_desgld () three_net_combined . append ( edesgld ) Accuracy Calculations extradesgld_acc = [] extradesgld_std = [] for i in tqdm ( range ( len ( nets ))) : exacc = [] exstd = [] for j in range ( len ( hv )) : extradesgld_accuracy = ClassificationAccuracy ( x_all , y_all , three_net_combined [ i ][ j ] , T ) acc , std = extradesgld_accuracy . compute_accuracy () exacc . append ( acc ) exstd . append ( std ) extradesgld_acc . append ( exacc ) extradesgld_std . append ( exstd ) extradesgld_acc = np . array ( extradesgld_acc ) extradesgld_std = np . array ( extradesgld_std ) Plot of the EXTRA DE-SGLD for each of the h values fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 8.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles =[ 'Fully Connected','Circular','Disconnected' ] for j , title in enumerate ( titles ) : for i , h_value in enumerate ( hv ) : axs [ j ] . plot ( index , extradesgld_acc [ j ][ i ] , label = f 'h = {h_value:.3f}' ) axs [ j ] . set_title ( title , fontsize = 24 ) axs [ j ] . legend ( loc = 'lower right' ) axs [ j ] . set_xlabel ( r 'Iterations $k$' , fontsize = 20 ) axs [ j ] . set_ylabel ( 'Accuracy' , fontsize = 24 ) axs [ j ] . set_ylim ( 0 , 1 ) axs [ j ] . tick_params ( labelsize = 24 ) axs [ j ] . grid () plt . tight_layout () plt . show () Find the Maximum Accuracy for the optimal h values max_acc_arrays = [] max_acc_indices = [] for i in range ( len ( nets )) : max_acc_array = extradesgld_acc [ i ][ np.argmax(np.max(extradesgld_acc[i ] , axis = 1 )) ] max_acc_index = np . argmax ( np . max ( extradesgld_acc [ i ] , axis = 1 )) max_acc_arrays . append ( max_acc_array ) max_acc_indices . append ( max_acc_index ) max_acc_arrays = np . array ( max_acc_arrays ) max_acc_indices = np . array ( max_acc_indices ) max_hv_values = hv [ max_acc_indices ] max_acc_stds = np . zeros (( len ( nets ), ( T + 1 ))) for i in range ( len ( nets )) : max_acc_stds [ i ] = extradesgld_std [ i, max_acc_indices[i ] ] Next plot the maximum accuracy for the optimal h fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 6.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles =[ 'Fully Connected','Circular','Disconnected' ] for i , title in enumerate ( titles ) : axs [ i ] . plot ( max_acc_arrays [ i ] , 'r-' , linewidth = 3 , label = f 'h = {max_hv_values[i]:.3f}' ) axs [ i ] . fill_between ( index , max_acc_arrays [ i ]+ max_acc_stds [ i ] , max_acc_arrays [ i ]- max_acc_stds [ i ] , alpha = 0.25 , color = 'red' ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . legend ( loc = 'lower right' ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( 'Accuracy' , fontsize = 24 ) axs [ i ] . set_ylim ( 0 , 1 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . grid () plt . tight_layout () plt . show () Now we can compare the approximations from both of the Vanila and Extra DeSGLD algorithms fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 6.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles = [ 'Fully Connected', 'Circular', 'Disconnected' ] for i , title in enumerate ( titles ) : axs [ i ] . plot ( desgld_acc [ i ] , 'b-' , linewidth = 3 , label = f 'Vanila DESGLD' ) axs [ i ] . fill_between ( index , desgld_acc [ i ] + desgld_std [ i ] , desgld_acc [ i ] - desgld_std [ i ] , alpha = 0.25 , color = 'blue' ) axs [ i ] . plot ( max_acc_arrays [ i ] , 'r-' , linewidth = 3 , label = f 'EXTRA DESGLD, h = {max_hv_values[i]:.3f}' ) axs [ i ] . fill_between ( index , max_acc_arrays [ i ] + max_acc_stds [ i ] , max_acc_arrays [ i ] - max_acc_stds [ i ] , alpha = 0.25 , color = 'red' ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . legend ( loc = 'lower right' ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( 'Accuracy' , fontsize = 24 ) axs [ i ] . set_ylim ( 0 , 1 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . grid () plt . tight_layout () plt . show ()","title":"Bayesian Logistic Regression with Synthetic Data"},{"location":"use/#bayesian-linear-regression-with-synthetic-data","text":"Parameters size_w=20; N=20; dim=2; sigma=np.eye(dim); sigma_sample=1; eta=0.009 ; T=50; lam=10; b=50; hv=np.linspace(0.001,0.5,5); Data Generation x=[] for i in range(1000): x.append([np.random.random()*1]) y=[item[0]*3-0.5+np.random.random() for item in x] x_all=np.array(x) y_all=np.array(y) x_all=np.insert(x_all, 0, 1, axis=1) cov_pri=lam*sigma avg_post=np.dot( np.linalg.inv(np.linalg.inv(cov_pri)+np.dot(np.transpose(x_all),x_all)/(sigma_sample**2)), (np.dot(np.transpose(x_all),y_all)/(sigma_sample**2)) ) cov_post=np.linalg.inv(np.linalg.inv(cov_pri)+np.dot(np.transpose(x_all),x_all)/(sigma_sample**2)) x=np.split(x_all,100) y=np.split(y_all,100) Vanila DeSGLD Approximation net=NetworkArchitecture(size_w) wf=net.fully_connected() wc=net.circular_network() wd=net.fully_disconnected() nets=np.array([wf,wc,wd]) Run the Vanila DeSGLD algorithm for all three networks three_net_combined_agents = [] three_net_combined_magents = [] for i in tqdm ( range ( len ( nets ))) : method = DeSGLD ( size_w , N , sigma , eta , T , dim , b , lam , x , y , nets [ i ] , hv , 'linear' ) desgld_agents , desgld_magents = method . vanila_desgld () three_net_combined_agents . append ( desgld_agents ) three_net_combined_magents . append ( desgld_magents ) Compute the w_2 distance desgld_three_net_combined_dist=[] for dis_agent,dis_magent in tqdm(zip(three_net_combined_agents,three_net_combined_magents)): distance=Wasserstein2Distance(size_w,T,avg_post,cov_post,dis_agent,dis_magent) dis=distance.W2_dist() desgld_three_net_combined_dist.append(dis) Next plot the distances fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 6.6 )) mpl . rcParams [ 'font.size' ] = 24 index = list ( range ( T + 1 )) titles =[ \"Fully Connected: DESGLD\",\"Circular: DESGLD\",\"Disconnected: DESGLD\" ] for i in range ( len ( desgld_three_net_combined_dist )) : axs [ i ] . plot ( index , desgld_three_net_combined_dist [ i ][ 0 ] , linewidth = 3 , label = r 'Agent 1 $x_1^{(k)}$' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ 1 ] , linewidth = 3 , label = r 'Agent 2 $x_2^{(k)}$' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ 2 ] , linewidth = 3 , label = r 'Agent 3 $x_3^{(k)}$' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ 3 ] , linewidth = 3 , label = r 'Agent 4 $x_4^{(k)}$' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ -1 ] , linewidth = 3 , label = r 'Mean of Agents $\\bar{x}^{(k)}$' ) axs [ i ] . set_title ( titles [ i ] , fontsize = 24 ) axs [ i ] . legend ( loc = 'upper right' , fontsize = 17 ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( r '$\\mathcal{W}_2$ Distance ' , fontsize = 24 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . grid () plt . tight_layout () plt . show () Here is the plot EXTRA DeSGLD Approximation Run the EXTRA DeSGLD algorithm for all three networks three_net_combined_agents = [] three_net_combined_magents = [] for i in tqdm ( range ( len ( nets ))) : method = DeSGLD ( size_w , N , sigma , eta , T , dim , b , lam , x , y , nets [ i ] , hv , 'linear' ) edesgld_agents , edesgld_magents = method . extra_desgld () three_net_combined_agents . append ( edesgld_agents ) three_net_combined_magents . append ( edesgld_magents ) Compute the w_2 Distances extradesgld_three_net_combined_dist = [] for i in tqdm ( range ( len ( nets ))) : ex = [] for j in range ( len ( hv )) : extradistance = Wasserstein2Distance ( size_w , T , avg_post , cov_post , three_net_combined_agents [ i ][ j ] , three_net_combined_magents [ i ][ j ] ) ex . append ( extradistance . W2_dist ()) extradesgld_three_net_combined_dist . append ( ex ) extradesgld_three_net_combined_dist = np . array ( extradesgld_three_net_combined_dist ) Work with the w_2 distances of mean agents mean_dist = np.empty((len(nets), len(hv), T+1)) for i in tqdm(range(len(nets))): for j in range(len(hv)): mean_dist[i, j, :] = extradesgld_three_net_combined_dist[i, j, -1, :] Plot the w_2 distance of mean agents for each h values fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 8.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ]= 24 titles =[ 'Fully Connected','Circular', 'Disconnected' ] for j , title in enumerate ( titles ) : for i , h_value in enumerate ( hv ) : axs [ j ] . plot ( mean_dist [ j ][ i ] , linewidth = 3 , label = f 'h={h_value:.3f}' ) axs [ j ] . set_title ( title , fontsize = 24 ) axs [ j ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ j ] . set_ylabel ( r '$\\mathcal{W}_2$ Dist. of mean agents' , fontsize = 24 ) axs [ j ] . tick_params ( labelsize = 24 ) axs [ j ] . legend ( loc = 'upper right' ) axs [ j ] . grid () plt . tight_layout () plt . show () Find the minimum distance for optimal h values min_dist_arrays = [] min_dist_indices = [] for i in range ( len ( nets )) : min_dist_array = mean_dist [ i ][ np.argmin(np.min(mean_dist[i ] , axis = 1 )) ] min_dist_index = np . argmin ( np . min ( mean_dist [ i ] , axis = 1 )) min_dist_arrays . append ( min_dist_array ) min_dist_indices . append ( min_dist_index ) min_dist_arrays = np . array ( min_dist_arrays ) min_dist_indices = np . array ( min_dist_indices ) min_hv_values = hv [ min_dist_indices ] min_hv_values Plot the Optimal EXTRA DeSGLD Mean Distance for the optimal h value fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 8.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ]= 24 titles =[ 'Fully Connected','Circular', 'Disconnected' ] for i , title in enumerate ( titles ) : axs [ i ] . plot ( min_dist_arrays [ i ] , 'r-' , linewidth = 3 , label = f 'h={min_hv_values[i]:.3f}' ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( r '$\\mathcal{W}_2$ Dist. of mean agents' , fontsize = 24 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . legend ( loc = 'upper right' ) axs [ i ] . grid () plt . tight_layout () plt . show () Finally, we can compare the performances from both of the algorithms fig , axs = plt . subplots ( 1 , 3 , figsize = ( 26.4 , 8.6 ), sharex = True , sharey = True ) mpl . rcParams [ 'font.size' ]= 24 titles =[ 'Fully Connected','Circular', 'Disconnected' ] for i , title in enumerate ( titles ) : axs [ i ] . plot ( min_dist_arrays [ i ] , 'r-' , linewidth = 3 , label = f 'EXTRA DESGLD for h={min_hv_values[i]:.3f}' ) axs [ i ] . plot ( desgld_three_net_combined_dist [ i ][ -1 ] , linewidth = 3 , label = 'Vanila DESGLD' ) axs [ i ] . set_title ( title , fontsize = 24 ) axs [ i ] . set_xlabel ( r 'Iterations $k$' , fontsize = 24 ) axs [ i ] . set_ylabel ( r '$\\mathcal{W}_2$ Dist. of mean agents' , fontsize = 24 ) axs [ i ] . tick_params ( labelsize = 24 ) axs [ i ] . legend ( loc = 'upper right' ) axs [ i ] . grid () plt . tight_layout () plt . show () And, here is the plot","title":"Bayesian Linear Regression with Synthetic Data"}]}